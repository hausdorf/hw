%Author Alex Clemmer
%CS 3810 Computer Organization
%Assignment 10:
\documentclass[a4paper]{article}
\usepackage[pdftex]{graphicx}
\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fullpage}
\addtolength{\oddsidemargin}{-.05in}
	\addtolength{\evensidemargin}{-.05in}
	\addtolength{\textwidth}{.25in}

	\addtolength{\textheight}{.25in}
\begin{document}

\section*{Assignment 10}
Alex Clemmer\\
Student number: u0458675

\section*{Problem 1:}

\paragraph{6.3.1} The book's example [pg. 577] contains a constant for controller overhead, while the question contains a "controller transfer rate". I will calculate exactly how I was told to by the TAs.

\begin{equation}
\begin{array}{llrll}
& \textbf{(a)} & 11.0 \mbox{ ms} + \cfrac{0.5 \mbox{ rotation}}{7,200 \mbox{ RPM}} + \cfrac{1 \mbox{ KB}}{34 \mbox{ MBytes/sec}} + \cfrac{1 \mbox{K B}}{480 \mbox{Mbits/sec}} & \approx & 15.1975 \mbox{ ms} \\[.15in]
& \textbf{(b)} & 9.0 \mbox{ ms} + \cfrac{0.5 \mbox{ rotation}}{7,200 \mbox{ RPM}} + \cfrac{1 \mbox{ KB}}{30 \mbox{ MBytes/sec}} + \cfrac{1 \mbox{KB}}{500 \mbox{ Mbits/sec}} & \approx & 13.2013 \mbox{ ms}
\end{array}
\end{equation}

\paragraph{6.3.2}
Our best time would contain no seek overhead and no rotational latency. Thus:

\begin{equation}
\begin{array}{llrll}
& \textbf{(a)} & \cfrac{2 \mbox{ KB}}{34 \mbox{ MBytes/sec}} + \cfrac{2 \mbox{ KB}}{480 \mbox{Mbits/sec}} & \approx & 0.061711519607 \mbox{ ms} \\[.15in]
& \textbf{(b)} & \cfrac{2 \mbox{ KB}}{30 \mbox{ MBytes/sec}} + \cfrac{2 \mbox{ KB}}{500 \mbox{Mbits/sec}} & \approx & 0.06920016666 \mbox{ ms}
\end{array}
\end{equation}

\paragraph{6.3.3} There is a clear and dominant factor here, and it is seek time. Whether or not we get data fast or slow is pretty much contingent upon how long it takes to seek. After that, the dominant factor is the rotational latency. Obviously we don't want to downplay how much time this takes compared to the other two factors, but even worst-case it still only takes around half the total time that an average-case seek would take.

Also worth noting is that increasing block size doesn't really change the read or write time by that much. In other words, for reasonably-sized blocks, the size does not appear to matter that much.

\section*{Problem 2:}

\paragraph{6.6.1} 

\begin{equation}
\begin{array}{llrll}
& \textbf{(a)} & \cfrac{1 \mbox{ KB}}{34 \mbox{ MBytes/sec}} + \cfrac{1 \mbox{ KB}}{480 \mbox{ Mbits/sec}} & \approx & 0.0308557598 \mbox{ ms} \\[.15in]
& \textbf{(b)} & \cfrac{1 \mbox{ KB}}{30 \mbox{ MBytes/sec}} + \cfrac{1 \mbox{KB}}{500 \mbox{MBits/sec}} & \approx & 0.03460008333 \mbox{ ms}
\end{array}
\end{equation}

\paragraph{6.6.2}

\begin{equation}
\begin{array}{llrll}
& \textbf{(a)} & \cfrac{0.5 \mbox{ KB}}{34 \mbox{ MBytes/sec}} + \cfrac{0.5 \mbox{ KB}}{480 \mbox{Mbits/sec}} & \approx & 0.0154279 \mbox{ ms} \\[.15in]
& \textbf{(b)} & \cfrac{0.5 \mbox{ KB}}{30 \mbox{ MBytes/sec}} + \cfrac{0.5 \mbox{ KB}}{500 \mbox{Mbits/sec}} & \approx & 0.0173000 \mbox{ ms}
\end{array}
\end{equation}

\paragraph{6.6.3} Obviously, the larger the memory, the more decode logic you have to wade through, not to mention the fact that propagating addresses around and through the Flash "array" obviously costs time. What may be surprising is that the dropoff appears to be so precipitous. Remember though, that in this example, the size increases by powers of two, while the dropoff does not decline at this rate. So the effects are not as severe as one might think.

\section*{Problem 3:}

\paragraph{3-A} So, first, the assumption I'm going to make is that one bit is sent per cycle. To compute this, we start by seeing how far a signal will travel in a single cycle:

\begin{equation}
\begin{array}{llrll}
& & \cfrac{2 \cdot 10^8 \mbox{ m/s}}{100 \cdot 10^6 \mbox{ MHz}} & = & 2 \mbox{ meters} \\[.15in]
\end{array}
\end{equation}

Then, we evaluate the "capacity" of the 10 meter wire. Assuming the rate of signal propagation across the wire is constant, that means that immediately as one is introduced at one end, another will disappear at the other end. Thus at any point there should be $\textbf{5 bits}$ moving across the wire.

\paragraph{3-B} The first thing we look at is how much wire the signal covers in one cycle:

\begin{equation}
\begin{array}{llrll}
& & \cfrac{2 \cdot 10^8 \mbox{ m/s}}{10 \cdot 10^9 \mbox{ MHz}} & = & 0.02 \mbox{ meters} \\[.15in]
\end{array}
\end{equation}

Using similar logic to above we get $\cfrac{1500 \mbox{ meters}}{0.02 \mbox{ meters/bit}} = 75,000 \mbox{ bits}$

\paragraph{3-C} This is tricky. First we'll do $\textbf{(a)}$. Let's start with what we know. There is 1 request of 100 bytes that is sent out. Since we are writing, there is an overhead of 0.03 ms. The other end must read, which also incurs this overhead. In other words, the signal gets sent to the receiver, and then the server takes 0.03 ms to process this on top of transmission time. At this point, our total should look like this:

\begin{equation}
0.03 \mbox{ ms} + \cfrac{100 \cdot 10.2 \mbox{ bits}}{100 \cdot 10^6 \mbox{ bits/sec}} + \cfrac{100 \cdot 2^{10} \mbox{ bits}}{100 \cdot 10^6} + 0.03 \mbox{ ms}
\end{equation}

Since I do not see anything about the amount of time it takes to find the data on disk or respond, I will not account for it here, although you should note that this is DEFINITELY NOT negligible. So, assuming that there is no data retrieval wait, we have another 0.03 ms that we must wait before we can write back in response.

Now we have to send the actual message. That's 100,000 bytes plus the overhead for each byte. And then at the end of the line, there's another read overhead. So, to our previous equation, we should add this:

\begin{equation}
... + 0.03 \mbox{ ms} + \cfrac{100,000 \cdot 10.2 \mbox{ bits}}{100 \cdot 10^6 \mbox{ bits/sec}} + \cfrac{100,000 \cdot 2^{10} \mbox{ bits}}{100 \cdot 10^6} + 0.03 \mbox{ ms}
\end{equation}

For $\textbf{(a)}$, this gives us $1.0352342$ seconds $+ 4 \cdot 0.03$ ms, for a $\textbf{total of 1.0353542}$.

For $\textbf{(b)}$, the same sort of derivation follows, except we changed the clock speed from $10^6$ to $10^9$. This gives us a total of $\textbf{total of 0.010472342}$. It's a lot smaller because the clock cycle is smaller.

\paragraph{3-D} It's important to note that between the two examples, the $\textit{only}$ difference is how fast we were sending the message out. This is pretty strong evidence that the main bottleneck is the frequency of our signal. If you want to go faster, increase how many cycles a second are going out.

I should also say that, while it's not a problem here, as you drop your cycle time, other bottlenecks may appear. If the R/W overhead of a real machine is more like a few milliseconds, or, say, you actually have to access memory, then you will start optimizing in other places.

\section*{Problem 4:}

\paragraph{7.6.1} So there are $(m \times p \times n)$ multiplications, which leaves us with $(m \times p \times (n-1))$ additions. We will be adding up the products, which means that we need to wait until at least two are available per addition. So, all things considered, in this case, the speedup should be in the ballpark of 4 times.

\paragraph{7.6.2} In this case, we are basically mapping different elements to the same cache line. Thus, each update ends up causing a cache miss, and so any speedup gained is diminished by a factor of 3 multiplied by the costs associated with a cache miss.

\paragraph{7.6.3} The simplest way is to traverse the matrix via columns instead of rows. That should pretty much eliminate the problem we were having because then the elements will end up mapped to different cache lines.

The only other thing I can think of -- and this is minor in comparison -- would be to process the matrix index $(i,j)$ and $(i+1,j)$ on the same core. Remember that we are dealing with false sharing, and thus this is important.

\section*{Problem 5:} First, the constraints. Once again, the additions depend on pairs of products, so in some ways we will be dependent on that factor. We obviously have overhead of $mx + mx$ to begin with.

That said, there are $(m \times p \times n)$ multiplications, and $(m \times p \times (n-1))$ additions. The multiplications are parallelizable, and after a few of them are done, so are the additions.

At some point, we can expect the number of multiplications to strictly limit how much more we can speed this all up by parallelization. In other words, this is in lot of ways this is the ideally parallel problem, and the "hard" limitations on speedup will come from limiting the number of operations there are to perform.

That is, apart from the distinct and well-defined constraints above, there will probably not be that many, unless you count hardware constraints ($\textit{e.g.}$, you have to run one OS per core on Intel's 80-core Polaris chip, because neither support that many cores; also there are issues with memory and things like that).

Because the task is so blatantly parallelizable, the speed up limitation should be roughly the number of operations that can possibly be parallelized divided by the number of cores. My guess would be $\cfrac{mpn + t \cdot mp(n-1) - 2mx}{x}$, where $t$ is some variable that accounts for the "problem" of addition.

What does that mean? Note that since addition is usually faster than multiplication (except in cases where the multiplication operation is strength-reduced, for example), the speedup afforded by parallelizing the addition will usually dependent on how fast the multiplication makes its operands available. That is, addition will execute as multiplication operations produce products to add together, and even then, the addition will finish generally before the next multiplication has, meaning that most of the speedup should be from how fast you can do the multiplication.

That of course is not to say that you can't speed up by parallelizing the addition! Certainly computing all of them at once is faster than doing them sequentially, but it is not freely parallelizable as the multiplication is.

\end{document}